# Описание проекта для презентации: Video Conference Platform

## Коротко о проекте (Problem → Solution)
**Задача:** быстро и удобно собирать людей в видеокомнате, с чатом и демонстрацией экрана — без лишних шагов для пользователя и без “монстра” по инфраструктуре.  
**Решение:** веб‑приложение видеоконференций. Backend отвечает за комнаты/участников/права доступа и интеграцию, а медиа‑часть (WebRTC) вынесена в **LiveKit** (SFU).

---

## Что реализовано (текущий функционал)
### Комнаты
- **Создание комнаты** (название, параметры)
- **Вход по ID комнаты** (поделиться ссылкой/ID — и человек внутри)
- **Получение media‑токена** для подключения к LiveKit

### Видеосвязь
- **Видео и аудио** (mute/unmute, camera on/off)
- **Несколько участников** в одной комнате
- **Демонстрация экрана**

### Чат
- **Текстовый чат** в комнате
- **Хранение сообщений в Redis** с TTL (история не хранится вечно — меньше рисков и дешевле хранение)

### Запуск/деплой
- **Docker Compose** поднимает весь стенд: PostgreSQL + Redis + LiveKit + Backend + Nginx
- Nginx раздает фронтенд и проксирует API

---

## Технологический стек
### Backend
- **Go 1.21**, **Gin**
- Архитектура по слоям: `handler → service → repository` (удобно развивать и сопровождать)

### Медиа
- **LiveKit (WebRTC, SFU)** — медиа‑сервер принимает и раздает потоки между участниками, снимая нагрузку с клиента
- **LiveKit Client SDK** на фронтенде

### Хранилища
- **PostgreSQL 15** — метаданные комнат/участников, долговременные сущности
- **Redis 7** — временные данные (чат, быстрые операции, TTL)

### Инфраструктура
- **Docker / Docker Compose**
- **Nginx** (reverse proxy + статика)

---

## Основная логика (как устроен основной флоу)
### 1) Создание комнаты
1. Клиент отправляет запрос в backend: создать комнату (title, max participants и т.п.)
2. Backend создает запись комнаты в PostgreSQL
3. Клиент получает `room_id` и может делиться им/ссылкой

### 2) Присоединение и запуск видео
1. Клиент делает `join` комнаты
2. Backend проверяет ограничения (например, лимит участников) и фиксирует участника
3. Backend выдает **LiveKit access token** на конкретную комнату
4. Клиент по токену подключается к LiveKit и публикует треки (audio/video/screen)

### 3) Чат
1. Клиент отправляет сообщение в backend
2. Backend валидирует данные + применяет rate limit
3. Сообщение сохраняется в Redis (с TTL), затем участники получают обновление (через API/WebSocket где используется)

---

## Какие баги/проблемы встречались и как решали
- **Ошибки БД (например, уникальность/дубликаты)**  
  Настроили корректную обработку ошибок PostgreSQL (включая `unique_violation`) и правильные HTTP‑статусы (409 вместо 500).

- **Валидация и “грязные” входные данные**  
  Добавили trim/нормализацию и ограничения на длину полей — меньше “странных” кейсов и падений на проде.

- **Конкурентные ситуации (race conditions)**  
  Учли моменты, когда несколько запросов одновременно создают/обновляют сущности. Решение — опора на ограничения БД и аккуратная логика в сервисном слое.

- **WebRTC‑нюансы в инфраструктуре**  
  Настройка портов/UDP и проксирование — важная часть для стабильной медиа‑связи, особенно при деплое.

---

## Что пока не идеально
- **Waiting Room** не реализованно
- **WebSocket** требует доводки (reconnect, устойчивость, нагрузка)
- **Устойчивость к нагрузкам** нужно провести нагрузочное тестирвоание для понимания потребляемых мощностей

---

## Масштабирование (как будем расти)
### Backend
- Backend **stateless** → легко масштабируется горизонтально (несколько инстансов за балансировщиком)
- Rate limiting: по IP / room / participant — защита от спама и злоупотреблений

### LiveKit
- Масштабирование медиа — через **кластеризацию LiveKit** и разнесение нагрузки по узлам
- При росте нагрузки медиа‑контур можно масштабировать отдельно от API

### Redis / PostgreSQL
- Redis: при росте чата — кластер/репликация, шардинг по `room_id`
- PostgreSQL: connection pooling, индексы на горячие запросы; при росте — партиционирование таблиц/архивация

### Наблюдаемость
- Метрики: latency/error rate API, Redis ops/memory, DB connections
- Трейсинг ключевых операций: create/join/token/chat

---

## Roadmap (следующие шаги)
- **Waiting Room + модерация** (роль в комнате, допуск/бан)
- **Нормальный real‑time чат** (WebSocket‑стабильность, backpressure, события)
- **UX‑улучшения** (список участников, статусы, настройки качества, устройства)
- **Прод‑готовность** (HTTPS, секреты, CI/CD, мониторинг)

---

## Итог (ценность)
Проект закрывает базовую потребность: **создать комнату → выдать доступ → запустить медиа и чат**.  
Архитектура разделяет ответственность: медиа живет в LiveKit, а backend управляет бизнес‑логикой и данными — поэтому систему можно устойчиво развивать и масштабировать.
